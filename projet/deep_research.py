import os
import json
from mistralai import Mistral
from selenium_util import scrape_worker_threaded
from google_search import fetch_search_results_with_googlesearch
from config import model, max_thread, max_synth_retries, max_retry_document
from concurrent.futures import ThreadPoolExecutor

# --- Global Configuration/State (mimicking parts of the class for clarity) ---
# These would ideally be managed by Streamlit's session_state or passed as arguments
# for standalone functions.
ALL_STEPS = [
    "Pr√©paration de la recherche",
    "G√©n√©ration des sous-recherches par l'IA",
    "R√©cup√©ration des URLs pour chaque sous-question",
    "Scraping des contenus des pages web",
    "Pr√©-r√©sum√© et validation des documents avec l'IA",
    "Validation de la pertinence des documents",
    "Recherche de documents de remplacement (si n√©cessaire)",
    "Synth√®se finale avec l'IA",
    "V√©rification de la coh√©rence de la synth√®se",
    "Recherche termin√©e"
]
TOTAL_STEPS = len(ALL_STEPS)


def _request_model(messages):
    client = Mistral(api_key=os.environ["MISTRAL_API_KEY"])
    resp = client.chat.complete(
        model=os.environ.get("MODEL", model),
        messages=messages,
    )
    raw_content = resp.choices[0].message.content.strip()
    return raw_content


def _validate_synthesis(query, synthesis, documents):
    system_prompt = """Tu es un assistant de validation d'information expert.
    Ta mission est d'√©valuer la qualit√© d'une synth√®se en la comparant aux documents sources qui ont servi √† la g√©n√©rer.
    R√©ponds uniquement avec un objet JSON strict.
    La synth√®se peut parler d'autre chose tant que c'est en rapport avec la question initiale et que la question initiale est r√©pondue dans la synthese.
    Si la synth√®se est pertinente et ne contient pas d'informations qui ne sont pas dans les documents sources, retourne : {"is_coherent": true, "reason": "La synth√®se est coh√©rente."}.
    Si la synth√®se est incoh√©rente, qu'elle "hallucine" ou qu'elle ne r√©pond pas √† la question, retourne : {"is_coherent": false, "reason": "Explique pourquoi la synth√®se n'est pas coh√©rente ou pertinente."}.
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",
         "content": f"""Question initiale : {query}\n\nSynth√®se √† valider : {synthesis}\n\nDocuments sources : {json.dumps(documents, ensure_ascii=False)}"""}
    ]
    try:
        response_json_str = _request_model(messages)
        response_json_str = response_json_str.strip().strip('`').strip('json').strip()
        return json.loads(response_json_str)
    except (json.JSONDecodeError, Exception) as e:
        print(f"‚ùå Erreur de format JSON ou de traitement lors de la validation : {e}")
        print("--- R√©ponse brute du mod√®le ---")
        print(response_json_str)
        print("------------------------------")
        return {"is_coherent": False, "reason": "Erreur de format ou de traitement de la r√©ponse de validation."}


def generate_subqueries_for_ui(query: str, k_pick: int, progress_callback) -> list[str]:
    """
    Generates subqueries and returns them to the UI.
    """
    current_step_idx = 1
    progress_callback(5, ALL_STEPS[current_step_idx], current_step_idx)

    system_msg = {
        "role": "system",
        "content": "Tu es un assistant expert en recherche documentaire."
    }
    user_msg = {
        "role": "user",
        "content": (
            f"Pour la question suivante :\n\n'{query}'\n\n"
            f"Merci de g√©n√©rer exactement {k_pick} sous-questions diff√©rentes, pr√©cises et pertinentes "
            "pour explorer ce sujet en profondeur. "
            "R√©ponds uniquement par une liste num√©rot√©e, sans aucune explication ni introduction, de cette forme :\n"
            "1. Premi√®re sous-question\n2. Deuxi√®me...\n3. Troisi√®me...\n"
        )
    }
    messages = [system_msg, user_msg]
    raw_text = _request_model(messages)
    subqueries = []
    for line in raw_text.splitlines():
        line = line.strip()
        if line.startswith(tuple(f"{i}." for i in range(1, k_pick + 1))):
            parts = line.split(".", 1)
            if len(parts) > 1:
                subqueries.append(parts[1].strip())
    if len(subqueries) < k_pick:
        print(f"‚ö†Ô∏è Seulement {len(subqueries)} sous-questions extraites sur {k_pick} attendues.")
        print("R√©ponse brute :", raw_text)
        subqueries = [query] * k_pick  # Fallback to main query

    print("üí° Sous-recherches g√©n√©r√©es :")
    for i, sq in enumerate(subqueries, 1):
        print(f"({i}) {sq}")
    return subqueries


def perform_full_research(
        query: str,
        subqueries: list[str],
        n_results: int,
        progress_callback
) -> dict:
    """
    Performs the full research workflow: fetching, scraping, validation, and synthesis.
    Returns the final result dictionary.
    """
    documents_by_subq = {sq: [] for sq in subqueries}
    visited_urls = set()
    validated_summaries = []
    final_synthesis = None

    # --- Fetch and Scrape ---
    current_step_idx = 2  # Starting step for this function
    progress_callback(20, ALL_STEPS[current_step_idx], current_step_idx)

    tasks = []
    for i, subq in enumerate(subqueries, 1):
        results_to_fetch = n_results + 3
        all_results = fetch_search_results_with_googlesearch(subq, results_to_fetch)
        urls_collected = 0
        print(f"URLs r√©cup√©r√©es pour sous-question {i} :")
        for title, url in all_results:
            if urls_collected >= n_results:
                break
            if url not in visited_urls:
                print(f"  {urls_collected + 1}. {url}")
                tasks.append({"url": url, "subquestion": subq})
                visited_urls.add(url)
                urls_collected += 1
            else:
                print(f"  {urls_collected + 1}. {url} (Ignor√© - doublon)")
    print()

    if not tasks:
        raise Exception("‚ùå Aucune URL valide √† scraper.")

    current_step_idx = 3
    max_workers = min(len(tasks), max_thread)
    progress_callback(40, ALL_STEPS[current_step_idx], current_step_idx)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(scrape_worker_threaded, task) for task in tasks]
        for future in futures:
            result = future.result()
            if result:
                documents_by_subq[result['subquestion']].append(result)

    print("\nüßæ Contenu extrait depuis les sites :")
    doc_count = 1
    for subq, docs in documents_by_subq.items():
        for doc in docs:
            print(f"--- Document {doc_count} ---")
            print(f"üîó URL : {doc.get('url')}")
            print(f"‚ùì Sous-question : {doc.get('subquestion')}")
            print(f"üè∑Ô∏è Titre : {doc.get('title')}")
            print("üìÑ Contenu extrait :")
            print(doc.get("paragraphs", "")[:500])
            print("----------------------\n")
            doc_count += 1

    # --- Validate and Summarize Documents ---
    current_step_idx = 4
    progress_callback(60, ALL_STEPS[current_step_idx], current_step_idx)

    system_prompt_validation = """Tu es un assistant de synth√®se expert. Pour le texte que je te donne, tu dois effectuer deux t√¢ches :
1.  G√©n√©rer un r√©sum√© pertinent d'environ 150 mots.
2.  √âvaluer sa pertinence par rapport √† la question initiale ( est ce que le document repond a la question initiale ou peut aider a r√©pondre a lquestion initiale avec l'aide d'autre document ).
R√©ponds uniquement avec un objet JSON strict.
Si le texte est pertinent, retourne : {"summary": "ton r√©sum√© ici", "is_relevant": true}.
Si le texte est hors sujet ou trop court, retourne : {"summary": null, "is_relevant": false}.
"""

    for subq_idx, subq in enumerate(subqueries):
        progress_for_subq = 60 + int(20 * (subq_idx / len(subqueries)))
        progress_callback(progress_for_subq, f"{ALL_STEPS[current_step_idx]} : {subq}", current_step_idx)

        print(f"\n--- Traitement de la sous-question : {subq} ---")
        relevant_docs = []
        subq_retries = 0

        initial_docs = documents_by_subq.get(subq, [])
        for doc in initial_docs:
            if len(relevant_docs) >= n_results:
                break

            if len(doc.get("paragraphs", "")) > 100:
                messages = [
                    {"role": "system", "content": system_prompt_validation},
                    {"role": "user",
                     "content": f"""Question initiale : {query}\nSous-question : {subq}\n\nTexte √† analyser :\n{doc.get('paragraphs')}"""}
                ]
                try:
                    response_json_str = _request_model(messages)
                    response_json_str = response_json_str.strip().strip('`').strip('json').strip()
                    response_data = json.loads(response_json_str)

                    if response_data.get("is_relevant", False):
                        relevant_docs.append({
                            "url": doc.get("url"),
                            "subquestion": doc.get("subquestion"),
                            "title": doc.get("title"),
                            "summary": response_data.get("summary")
                        })
                        print(f"‚úÖ Document pertinent trouv√© : {doc.get('url')}")
                    else:
                        print(f"‚ö†Ô∏è Document de {doc.get('url')} jug√© non pertinent.")

                except (json.JSONDecodeError, Exception) as e:
                    print(f"‚ùå Erreur de format JSON ou de traitement pour {doc.get('url')} : {e}")
                    print("--- R√©ponse brute du mod√®le ---")
                    print(response_json_str)
                    print("------------------------------")
            else:
                if doc.get("paragraphs", "").strip():
                    relevant_docs.append({
                        "url": doc.get("url"),
                        "subquestion": doc.get("subquestion"),
                        "title": doc.get("title"),
                        "summary": doc.get("paragraphs")
                    })
                    print(f"‚úÖ Document court mais pertinent trouv√© : {doc.get('url')}")
                else:
                    print(f"‚ö†Ô∏è Document de {doc.get('url')} est vide. Ignor√©.")

        current_step_idx_retry = 6
        while len(relevant_docs) < n_results and subq_retries < max_retry_document:
            progress_callback(progress_for_subq,
                              f"{ALL_STEPS[current_step_idx_retry]} : {subq} (tentative {subq_retries + 1})",
                              current_step_idx_retry)
            print(
                f"‚ôªÔ∏è Nombre de documents pertinents insuffisant pour '{subq}'. Recherche d'une URL de remplacement (tentative {subq_retries + 1}/{max_retry_document})....")

            new_results_to_check = fetch_search_results_with_googlesearch(subq, 10)

            new_url_found = False
            for new_title, new_url in new_results_to_check:
                if new_url not in visited_urls:
                    print(f"‚úÖ Nouvelle URL de remplacement trouv√©e : {new_url}")
                    visited_urls.add(new_url)
                    new_url_found = True

                    new_doc = scrape_worker_threaded({"url": new_url, "subquestion": subq})

                    if new_doc and len(new_doc.get("paragraphs", "")) > 100:
                        try:
                            messages = [
                                {"role": "system", "content": system_prompt_validation},
                                {"role": "user",
                                 "content": f"""Question initiale : {query}\nSous-question : {subq}\n\nTexte √† analyser :\n{new_doc.get('paragraphs')}"""}
                            ]
                            response_json_str = _request_model(messages)
                            response_json_str = response_json_str.strip().strip('`').strip('json').strip()
                            response_data = json.loads(response_json_str)
                            if response_data.get("is_relevant", False):
                                relevant_docs.append({
                                    "url": new_doc.get("url"),
                                    "subquestion": new_doc.get("subquestion"),
                                    "title": new_doc.get("title"),
                                    "summary": response_data.get("summary")
                                })
                                print(f"‚úÖ Document de remplacement pertinent trouv√© et ajout√©.")
                            else:
                                print(f"‚ö†Ô∏è Document de {new_doc.get('url')} jug√© non pertinent.")
                        except Exception as e:
                            print(f"‚ùå Erreur de traitement du document de remplacement : {e}.")
                    else:
                        print(f"‚ö†Ô∏è Document de {new_doc.get('url')} est vide ou trop court. Ignor√©.")

                    break

            if not new_url_found:
                print(
                    "‚ùå Aucune nouvelle URL valide √† ajouter. Arr√™t de la recherche de remplacement pour cette sous-question.")
                break

            subq_retries += 1

        validated_summaries.extend(relevant_docs)

    if not validated_summaries:
        raise Exception(f"\n‚ùå La validation des documents n'a pas abouti.")

    # --- Synthesize Final Answer ---
    current_step_idx = 7
    progress_callback(85, ALL_STEPS[current_step_idx], current_step_idx)
    synth = None
    attempt = 0
    while synth is None and attempt < max_synth_retries:
        attempt += 1
        print(f"Tentative de synth√®se n¬∞{attempt}/{max_synth_retries}...")
        synth_msgs = [
            {"role": "system",
             "content": "Tu es un assistant de synth√®se expert. Tu vas synth√©tiser les informations contenues dans les r√©sum√©s pertinents fournis pour r√©pondre √† la question initiale. Utilise les URLs comme r√©f√©rences. Et ne met pas de partie de reference."},
            {"role": "user",
             "content": json.dumps({"query": query, "data": validated_summaries}, ensure_ascii=False)}
        ]
        synth_content = _request_model(synth_msgs)

        current_step_idx_validation = 8
        progress_callback(90, ALL_STEPS[current_step_idx_validation], current_step_idx_validation)
        print("\n‚úÇÔ∏è √âtape 4 : v√©rification de la coh√©rence de la synth√®se...")
        validation_result = _validate_synthesis(query, synth_content, validated_summaries)

        if validation_result.get("is_coherent", False):
            print("‚úÖ La synth√®se est coh√©rente et valide.")
            synth = synth_content
        else:
            print(f"‚ö†Ô∏è La synth√®se est jug√©e incoh√©rente. Raison : {validation_result.get('reason')}")
            if attempt < max_synth_retries:
                print("‚ôªÔ∏è Nouvelle tentative de synth√®se...")
            else:
                print("‚ùå Nombre maximal de tentatives de synth√®se atteint. Renvoie de la derniere synthese.")
                synth = synth_content
                break
    final_synthesis = synth
    if not final_synthesis:
        raise Exception("La synth√®se finale n'a pas pu √™tre g√©n√©r√©e.")

    current_step_idx = 9
    progress_callback(100, ALL_STEPS[current_step_idx], current_step_idx)

    # --- Prepare Final Result ---
    sources_by_subquery = []
    for sq in subqueries:
        urls = [doc["url"] for doc in validated_summaries if doc.get("subquestion") == sq]
        sources_by_subquery.append({
            "subquestion": sq,
            "urls": urls
        })
    return {
        "synth√®se": final_synthesis,
        "sous_questions": subqueries,
        "sources": sources_by_subquery,
    }