import os
import json
import argparse
from mistralai import Mistral
from selenium_util import scrape_worker_threaded
from google_search import fetch_search_results_with_googlesearch
from config import model, max_thread, max_synth_retries, max_retry_document
from concurrent.futures import ThreadPoolExecutor

def parse_args():
    parser = argparse.ArgumentParser(description="Recherche profonde avec s√©lection et synth√®se")
    parser.add_argument("query", help="Question ou sujet √† rechercher")
    parser.add_argument("-n", "--n_results", type=int, default=5,
                        help="Nombre de r√©sultats Google √† r√©cup√©rer par sous-question")
    parser.add_argument("-k", "--k_pick", type=int, default=3, help="Nombre de sous-questions √† g√©n√©rer")
    return parser.parse_args()

def request_model(messages):
    client = Mistral(api_key=os.environ["MISTRAL_API_KEY"])
    resp = client.chat.complete(
        model=os.environ.get("MODEL", model),
        messages=messages,
    )
    raw_content = resp.choices[0].message.content.strip()
    return raw_content

def generate_subqueries(query, k):
    system_msg = {
        "role": "system",
        "content": "Tu es un assistant expert en recherche documentaire."
    }
    user_msg = {
        "role": "user",
        "content": (
            f"Pour la question suivante :\n\n'{query}'\n\n"
            f"Merci de g√©n√©rer exactement {k} sous-questions diff√©rentes, pr√©cises et pertinentes "
            "pour explorer ce sujet en profondeur. "
            "R√©ponds uniquement par une liste num√©rot√©e, sans aucune explication ni introduction, de cette forme :\n"
            "1. Premi√®re sous-question\n2. Deuxi√®me...\n3. Troisi√®me...\n"
        )
    }
    messages = [system_msg, user_msg]
    raw_text = request_model(messages)
    subqueries = []
    for line in raw_text.splitlines():
        line = line.strip()
        if line.startswith(tuple(f"{i}." for i in range(1, k + 1))):
            parts = line.split(".", 1)
            if len(parts) > 1:
                subqueries.append(parts[1].strip())
    if len(subqueries) < k:
        print(f"‚ö†Ô∏è Seulement {len(subqueries)} sous-questions extraites sur {k} attendues.")
        print("R√©ponse brute :", raw_text)
        subqueries = [query] * k
    return subqueries

def validate_synthesis(query, synthesis, documents):
    system_prompt = """Tu es un assistant de validation d'information expert.
    Ta mission est d'√©valuer la qualit√© d'une synth√®se en la comparant aux documents sources qui ont servi √† la g√©n√©rer.
    R√©ponds uniquement avec un objet JSON strict.
    La synth√®se peut parler d'autre chose tant que c'est en rapport avec la question initiale et que la question initiale est r√©pondue dans la synthese.
    Si la synth√®se est pertinente et ne contient pas d'informations qui ne sont pas dans les documents sources, retourne : {"is_coherent": true, "reason": "La synth√®se est coh√©rente."}.
    Si la synth√®se est incoh√©rente, qu'elle "hallucine" ou qu'elle ne r√©pond pas √† la question, retourne : {"is_coherent": false, "reason": "Explique pourquoi la synth√®se n'est pas coh√©rente ou pertinente."}.
    """
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",
         "content": f"""Question initiale : {query}\n\nSynth√®se √† valider : {synthesis}\n\nDocuments sources : {json.dumps(documents, ensure_ascii=False)}"""}
    ]
    try:
        response_json_str = request_model(messages)
        response_json_str = response_json_str.strip().strip('`').strip('json').strip()
        return json.loads(response_json_str)
    except (json.JSONDecodeError, Exception) as e:
        print(f"‚ùå Erreur de format JSON ou de traitement lors de la validation : {e}")
        print("--- R√©ponse brute du mod√®le ---")
        print(response_json_str)
        print("------------------------------")
        return {"is_coherent": False, "reason": "Erreur de format ou de traitement de la r√©ponse de validation."}

def deep_research(query, n_results, k_pick, progress_callback=None):
    if progress_callback is None:
        progress_callback = lambda p, s: None # Default do-nothing callback

    max_global_retries = 3
    global_retry_count = 0
    final_result = None

    while global_retry_count < max_global_retries:
        print(f"\n--- D√©but de la tentative de recherche globale n¬∞{global_retry_count + 1}/{max_global_retries} ---")
        progress_callback(5, "üß† G√©n√©ration des sous-recherches par l'IA...")
        subqueries = generate_subqueries(query, k_pick)
        print("üí° Sous-recherches g√©n√©r√©es :")
        for i, sq in enumerate(subqueries, 1):
            print(f"({i}) {sq}")
        print()

        documents_by_subq = {sq: [] for sq in subqueries}
        visited_urls = set()
        tasks = []

        progress_callback(20, "üîç R√©cup√©ration des URLs pour chaque sous-question...")
        for i, subq in enumerate(subqueries, 1):
            results_to_fetch = n_results + 3
            all_results = fetch_search_results_with_googlesearch(subq, results_to_fetch)
            urls_collected = 0
            print(f"URLs r√©cup√©r√©es pour sous-question {i} :")
            for title, url in all_results:
                if urls_collected >= n_results:
                    break
                if url not in visited_urls:
                    print(f"  {urls_collected + 1}. {url}")
                    tasks.append({"url": url, "subquestion": subq})
                    visited_urls.add(url)
                    urls_collected += 1
                else:
                    print(f"  {urls_collected + 1}. {url} (Ignor√© - doublon)")
        print()

        if not tasks:
            print("‚ùå Aucune URL valide √† scraper. Tentative suivante...")
            global_retry_count += 1
            continue # Restart the while loop for a new global retry

        max_workers = min(len(tasks), max_thread)
        progress_callback(40, f"üöÄ D√©marrage du scraping en parall√®le avec {max_workers} threads...")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(scrape_worker_threaded, task) for task in tasks]
            for future in futures:
                result = future.result()
                if result:
                    documents_by_subq[result['subquestion']].append(result)

        print("\nüßæ Contenu extrait depuis les sites :")
        doc_count = 1
        for subq, docs in documents_by_subq.items():
            for doc in docs:
                print(f"--- Document {doc_count} ---")
                print(f"üîó URL : {doc.get('url')}")
                print(f"‚ùì Sous-question : {doc.get('subquestion')}")
                print(f"üè∑Ô∏è Titre : {doc.get('title')}")
                print("üìÑ Contenu extrait :")
                print(doc.get("paragraphs", "")[:500])
                print("----------------------\n")
                doc_count += 1

        progress_callback(60, "‚úÇÔ∏è Pr√©-r√©sum√© et validation des documents avec l'IA...")
        validated_summaries = []

        for subq_idx, subq in enumerate(subqueries):
            progress_callback(60 + int(20 * (subq_idx / len(subqueries))), f"‚úÇÔ∏è Traitement de la sous-question : {subq}")
            print(f"\n--- Traitement de la sous-question : {subq} ---")
            relevant_docs = []
            subq_retries = 0

            initial_docs = documents_by_subq.get(subq, [])
            for doc in initial_docs:
                if len(relevant_docs) >= n_results:
                    break

                if len(doc.get("paragraphs", "")) > 100:
                    system_prompt = """Tu es un assistant de synth√®se expert. Pour le texte que je te donne, tu dois effectuer deux t√¢ches :
1.  G√©n√©rer un r√©sum√© pertinent d'environ 150 mots.
2.  √âvaluer sa pertinence par rapport √† la question initiale ( est ce que le document repond a la question initiale ou peut aider a r√©pondre a lquestion initiale avec l'aide d'autre document ).
R√©ponds uniquement avec un objet JSON strict.
Si le texte est pertinent, retourne : {"summary": "ton r√©sum√© ici", "is_relevant": true}.
Si le texte est hors sujet ou trop court, retourne : {"summary": null, "is_relevant": false}.
"""
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user",
                         "content": f"""Question initiale : {query}\nSous-question : {subq}\n\nTexte √† analyser :\n{doc.get('paragraphs')}"""}
                    ]
                    try:
                        response_json_str = request_model(messages)
                        response_json_str = response_json_str.strip().strip('`').strip('json').strip()
                        response_data = json.loads(response_json_str)

                        if response_data.get("is_relevant", False):
                            relevant_docs.append({
                                "url": doc.get("url"),
                                "subquestion": doc.get("subquestion"),
                                "title": doc.get("title"),
                                "summary": response_data.get("summary")
                            })
                            print(f"‚úÖ Document pertinent trouv√© : {doc.get('url')}")
                        else:
                            print(f"‚ö†Ô∏è Document de {doc.get('url')} jug√© non pertinent.")

                    except (json.JSONDecodeError, Exception) as e:
                        print(f"‚ùå Erreur de format JSON ou de traitement pour {doc.get('url')} : {e}")
                        print("--- R√©ponse brute du mod√®le ---")
                        print(response_json_str)
                        print("------------------------------")
                else:
                    if doc.get("paragraphs", "").strip():
                        relevant_docs.append({
                            "url": doc.get("url"),
                            "subquestion": doc.get("subquestion"),
                            "title": doc.get("title"),
                            "summary": doc.get("paragraphs")
                        })
                        print(f"‚úÖ Document court mais pertinent trouv√© : {doc.get('url')}")
                    else:
                        print(f"‚ö†Ô∏è Document de {doc.get('url')} est vide. Ignor√©.")

            while len(relevant_docs) < n_results and subq_retries < max_retry_document:
                print(
                    f"‚ôªÔ∏è Nombre de documents pertinents insuffisant pour '{subq}'. Recherche d'une URL de remplacement (tentative {subq_retries + 1}/{max_retry_document})...")

                new_results_to_check = fetch_search_results_with_googlesearch(subq, 10)

                new_url_found = False
                for new_title, new_url in new_results_to_check:
                    if new_url not in visited_urls:
                        print(f"‚úÖ Nouvelle URL de remplacement trouv√©e : {new_url}")
                        visited_urls.add(new_url)
                        new_url_found = True

                        new_task = {"url": new_url, "subquestion": subq}
                        new_doc = scrape_worker_threaded(new_task)

                        if new_doc and len(new_doc.get("paragraphs", "")) > 100:
                            try:
                                messages = [
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user",
                                     "content": f"""Question initiale : {query}\nSous-question : {subq}\n\nTexte √† analyser :\n{new_doc.get('paragraphs')}"""}
                                ]
                                response_json_str = request_model(messages)
                                response_json_str = response_json_str.strip().strip('`').strip('json').strip()
                                response_data = json.loads(response_json_str)
                                if response_data.get("is_relevant", False):
                                    relevant_docs.append({
                                        "url": new_doc.get("url"),
                                        "subquestion": new_doc.get("subquestion"),
                                        "title": new_doc.get("title"),
                                        "summary": response_data.get("summary")
                                    })
                                    print(f"‚úÖ Document de remplacement pertinent trouv√© et ajout√©.")
                                else:
                                    print(f"‚ö†Ô∏è Document de {new_doc.get('url')} jug√© non pertinent.")
                            except Exception as e:
                                print(f"‚ùå Erreur de traitement du document de remplacement : {e}.")
                        else:
                            print(f"‚ö†Ô∏è Document de remplacement vide ou trop court. Ignor√©.")

                        break

                if not new_url_found:
                    print("‚ùå Aucune nouvelle URL valide √† ajouter. Arr√™t de la recherche de remplacement pour cette sous-question.")
                    break

                subq_retries += 1

            validated_summaries.extend(relevant_docs)

        if not validated_summaries:
            print(f"\n‚ùå La validation des documents n'a pas abouti pour cette tentative. (Tentative {global_retry_count + 1}/{max_global_retries}). Red√©marrage de la recherche...")
            global_retry_count += 1
            continue # Restart the while loop for a new global retry

        progress_callback(85, "ü§ñ Synth√®se finale avec l'IA...")
        synth = None
        attempt = 0
        while synth is None and attempt < max_synth_retries:
            attempt += 1
            print(f"Tentative de synth√®se n¬∞{attempt}/{max_synth_retries}...")
            synth_msgs = [
                {"role": "system",
                 "content": "Tu es un assistant de synth√®se expert. Tu vas synth√©tiser les informations contenues dans les r√©sum√©s pertinents fournis pour r√©pondre √† la question initiale. Utilise les URLs comme r√©f√©rences. Et ne met pas de partie de reference."},
                {"role": "user", "content": json.dumps({"query": query, "data": validated_summaries}, ensure_ascii=False)}
            ]
            synth_content = request_model(synth_msgs)

            print("\n‚úÇÔ∏è √âtape 4 : v√©rification de la coh√©rence de la synth√®se...")
            validation_result = validate_synthesis(query, synth_content, validated_summaries)

            if validation_result.get("is_coherent", False):
                print("‚úÖ La synth√®se est coh√©rente et valide.")
                synth = synth_content
            else:
                print(f"‚ö†Ô∏è La synth√®se est jug√©e incoh√©rente. Raison : {validation_result.get('reason')}")
                if attempt < max_synth_retries:
                    print("‚ôªÔ∏è Nouvelle tentative de synth√®se...")
                else:
                    print("‚ùå Nombre maximal de tentatives de synth√®se atteint. Renvoie de la derniere synthese.")
                    synth = synth_content # Return the last synthesis, even if incoherent for final review
                    break # Exit inner while loop

        if synth: # If a synthesis was successfully generated (coherent or last attempt)
            sources_by_subquery = []
            for sq in subqueries:
                urls = [doc["url"] for doc in validated_summaries if doc.get("subquestion") == sq]
                sources_by_subquery.append({
                    "subquestion": sq,
                    "urls": urls
                })
            final_result = {
                "synth√®se": synth,
                "sous_questions": subqueries,
                "sources": sources_by_subquery,
            }
            progress_callback(100, "‚úÖ Recherche termin√©e !")
            return final_result # Successfully completed, exit the global retry loop

        else: # If synth is None (meaning no valid synthesis after max_synth_retries)
            print(f"\n‚ùå La synth√®se finale a √©chou√© apr√®s {max_synth_retries} tentatives. (Tentative globale {global_retry_count + 1}/{max_global_retries}). Red√©marrage de la recherche...")
            global_retry_count += 1
            continue # Restart the while loop for a new global retry

    # If we reach here, all global retries have been exhausted without a successful result
    print("\n‚ùå Toutes les tentatives de recherche globale ont √©chou√©.")
    progress_callback(100, "‚ùå Recherche √©chou√©e apr√®s plusieurs tentatives.")
    return final_result # Return the last attempt's result, which might be None or an incoherent synthesis

def main():
    args = parse_args()
    if "MISTRAL_API_KEY" not in os.environ:
        print("‚ùå La variable MISTRAL_API_KEY doit √™tre d√©finie.")
        exit(1)
    answer = deep_research(args.query, args.n_results, args.k_pick)
    if answer:
        print("\nüß† Synth√®se finale :\n", answer["synth√®se"])
        print("\nüîé Sous-questions explor√©es :")
        for sq in answer["sous_questions"]:
            print(f"- {sq}")
        print("\nüîó Sources utilis√©es :")
        for source_group in answer["sources"]:
            print(f"  - Pour '{source_group['subquestion']}':")
            for url in source_group['urls']:
                print(f"    - {url}")
    else:
        print("\n‚ùå Aucune r√©ponse valide n'a pu √™tre g√©n√©r√©e apr√®s plusieurs tentatives.")

if __name__ == "__main__":
    main()
